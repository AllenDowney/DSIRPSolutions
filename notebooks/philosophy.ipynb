{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaedff8",
   "metadata": {},
   "source": [
    "# Chapter xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40cc9f",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "*Data Structures and Information Retrieval in Python*\n",
    "\n",
    "Copyright 2021 Allen Downey\n",
    "\n",
    "License: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732c7ae2",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print('Downloaded ' + local)\n",
    "    \n",
    "# download('https://github.com/AllenDowney/DSIRP/raw/main/utils.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b883d41",
   "metadata": {},
   "source": [
    "[Click here to run this chapter on Colab](https://colab.research.google.com/github/AllenDowney/DSIRP/blob/main/chapters/chap01.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4a79b",
   "metadata": {},
   "source": [
    "# Getting to Philosophy\n",
    "\n",
    "The goal of this chapter is to develop a Web crawler that tests the\n",
    "\"Getting to Philosophy\" conjecture, which we presented in\n",
    "SectionÂ [\\[the-road-ahead\\]](#the-road-ahead){reference-type=\"ref\"\n",
    "reference=\"the-road-ahead\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d55b02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "(<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>),\n",
    "<i><a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and</i>\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61819a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18daf934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_doc)\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65f13165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_DFS(root):\n",
    "    stack = []\n",
    "    stack.append(root)\n",
    "    \n",
    "    while(stack):\n",
    "        tag = stack.pop()\n",
    "        yield tag\n",
    "\n",
    "        children = getattr(tag, \"contents\", [])\n",
    "        for child in reversed(children):\n",
    "            stack.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53eafee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and ((their) names) were\n",
      "(Elsie),\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import NavigableString\n",
    "\n",
    "for element in iterative_DFS(soup):\n",
    "    if isinstance(element, NavigableString):\n",
    "        print(element.string, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa828190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import Tag\n",
    "\n",
    "def link_generator(root):\n",
    "    paren_stack = []\n",
    "\n",
    "    for element in iterative_DFS(root):\n",
    "        if isinstance(element, NavigableString):\n",
    "            for char in element.string:\n",
    "                if char == '(':\n",
    "                    paren_stack.append(char)\n",
    "                if char == ')':\n",
    "                    paren_stack.pop()\n",
    "\n",
    "        if isinstance(element, Tag) and element.name == \"a\":\n",
    "            if len(paren_stack):\n",
    "                continue\n",
    "            yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41067025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object link_generator at 0x7fdae01973c0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = link_generator(soup)\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d78b1c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = next(it)\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "af48fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_bad_tag(element, bad_tags=['i', 'table']):\n",
    "    if isinstance(element, BeautifulSoup):\n",
    "        return False\n",
    "    if isinstance(element, Tag) and element.name in bad_tags:\n",
    "        return True\n",
    "    return in_bad_tag(element.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "93f8f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n"
     ]
    }
   ],
   "source": [
    "for link in link_generator(soup):\n",
    "    if in_bad_tag(link):\n",
    "        continue\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e60fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9c97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95309299",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "41df13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "\n",
    "filename = basename(url)\n",
    "fp = open(filename)\n",
    "soup2 = BeautifulSoup(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "31215d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = soup2.find(class_='mw-body-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2c2e4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_link_generator(root):\n",
    "    for link in link_generator(root):\n",
    "        if in_bad_tag(link):\n",
    "            continue\n",
    "            \n",
    "        href = link.get(\"href\", '')\n",
    "        if not href.startswith('/wiki'):\n",
    "            continue\n",
    "\n",
    "        class_ = link.get(\"class\", '')\n",
    "        if \"mw-disambig\" in class_:\n",
    "            continue\n",
    "            \n",
    "        yield link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f9fd3f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"mw-redirect\" href=\"/wiki/Interpreted_language\" title=\"Interpreted language\">interpreted</a>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = valid_link_generator(root)\n",
    "link = next(it)\n",
    "link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f73b3",
   "metadata": {},
   "source": [
    "## `WikiFetcher`\n",
    "\n",
    "When you write a Web crawler, it is easy to download too many pages too\n",
    "fast, which might violate the terms of service for the server you are\n",
    "downloading from. To help you avoid that, I provide a class called\n",
    "`WikiFetcher` that does two things:\n",
    "\n",
    "1.  It encapsulates the code we demonstrated in the previous chapter for\n",
    "    downloading pages from Wikipedia, parsing the HTML, and selecting\n",
    "    the content text.\n",
    "\n",
    "2.  It measures the time between requests and, if we don't leave enough\n",
    "    time between requests, it sleeps until a reasonable interval has\n",
    "    elapsed. By default, the interval is one second.\n",
    "\n",
    "Here's the definition of `WikiFetcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6c5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time, sleep\n",
    "    \n",
    "class WikiFetcher:\n",
    "    next_request_time = None\n",
    "    min_interval = 1  # second\n",
    "\n",
    "    def fetch_wikipedia(self, url):\n",
    "        self.sleep_if_needed()\n",
    "        fp = urlopen(url)\n",
    "        soup = BeautifulSoup(fp, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    def sleep_if_needed(self):\n",
    "        if self.next_request_time:\n",
    "            sleep_time = self.next_request_time - time()    \n",
    "            if sleep_time > 0:\n",
    "                sleep(sleep_time)\n",
    "        \n",
    "        self.next_request_time = time() + self.min_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b6528",
   "metadata": {},
   "source": [
    "The only public method is `fetchWikipedia`, which takes a URL as a\n",
    "`String` and returns an `Elements` collection that contains one DOM\n",
    "element for each paragraph in the content text. This code should look\n",
    "familiar.\n",
    "\n",
    "The new code is in `sleepIfNeeded`, which checks the time since the last\n",
    "request and sleeps if the elapsed time is less than `minInterval`, which\n",
    "is in milliseconds.\n",
    "\n",
    "That's all there is to `WikiFetcher`. Here's an example that\n",
    "demonstrates how it's used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97c92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = WikiFetcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ad5188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1627917177.0105095\n",
      "1627917177.4788275\n",
      "1627917178.482072\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "\n",
    "print(time())\n",
    "wf.fetch_wikipedia(url)\n",
    "print(time())\n",
    "wf.fetch_wikipedia(url)\n",
    "print(time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335b9a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66332ab6",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "In `WikiPhilosophy.java` you'll find a simple `main` method that shows\n",
    "how to use some of these pieces. Starting with this code, your job is to\n",
    "write a crawler that:\n",
    "\n",
    "1.  Takes a URL for a Wikipedia page, downloads it, and parses it.\n",
    "\n",
    "2.  It should traverse the resulting DOM tree to find the first *valid*\n",
    "    link. I'll explain what \"valid\" means below.\n",
    "\n",
    "3.  If the page has no links, or if the first link is a page we have\n",
    "    already seen, the program should indicate failure and exit.\n",
    "\n",
    "4.  If the link matches the URL of the Wikipedia page on philosophy, the\n",
    "    program should indicate success and exit.\n",
    "\n",
    "5.  Otherwise it should go back to Step 1.\n",
    "\n",
    "The program should build a `List` of the URLs it visits and display the\n",
    "results at the end (whether it succeeds or fails).\n",
    "\n",
    "So what should we consider a \"valid\" link? You have some choices here.\n",
    "Various versions of the \"Getting to Philosophy\" conjecture use slightly\n",
    "different rules, but here are some options:\n",
    "\n",
    "1.  The link should be in the content text of the page, not in a sidebar\n",
    "    or boxout.\n",
    "\n",
    "2.  It should not be in italics or in parentheses.\n",
    "\n",
    "3.  You should skip external links, links to the current page, and red\n",
    "    links.\n",
    "\n",
    "4.  In some versions, you should skip a link if the text starts with an\n",
    "    uppercase letter.\n",
    "\n",
    "You don't have to enforce all of these rules, but we recommend that you\n",
    "at least handle parentheses, italics, and links to the current page.\n",
    "\n",
    "If you feel like you have enough information to get started, go ahead.\n",
    "Or you might want to read these hints:\n",
    "\n",
    "1.  As you traverse the tree, the two kinds of `Node` you will need to\n",
    "    deal with are `TextNode` and `Element`. If you find an `Element`,\n",
    "    you will probably have to typecast it to access the tag and other\n",
    "    information.\n",
    "\n",
    "2.  When you find an `Element` that contains a link, you can check\n",
    "    whether it is in italics by following parent links up the tree. If\n",
    "    there is an `<i>` or `<em>` tag in the parent chain, the link is in\n",
    "    italics.\n",
    "\n",
    "3.  To check whether a link is in parentheses, you will have to scan\n",
    "    through the text as you traverse the tree and keep track of opening\n",
    "    and closing parentheses (ideally your solution should be able to\n",
    "    handle nested parentheses (like this)).\n",
    "\n",
    "4.  If you start from the Java page, you should get to Philosophy after\n",
    "    following seven links, unless something has changed since I ran the\n",
    "    code.\n",
    "\n",
    "OK, that's all the help you're going to get. Now it's up to you. Have\n",
    "fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
